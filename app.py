import gradio as gr
import requests
import json
import os
from typing import List, Tuple

class JapaneseLLMChat:
    def __init__(self):
        # åˆ©ç”¨å¯èƒ½ãªLLMãƒ¢ãƒ‡ãƒ«ï¼ˆInference APIå¯¾å¿œç¢ºèªæ¸ˆã¿ï¼‰
        self.models = {
            # æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆInference APIå¯¾å¿œï¼‰
            "cyberagent/open-calm-7b": "CyberAgent Open CALM 7B",
            "rinna/japanese-gpt-neox-3.6b-instruction-sft": "Rinna GPT-NeoX 3.6B",
            "matsuo-lab/weblab-10b-instruction-sft": "Matsuo Lab WebLab 10B",
            "stabilityai/japanese-stablelm-instruct-alpha-7b": "Japanese StableLM 7B",
            "tokyotech-llm/Swallow-7b-instruct-hf": "Swallow 7B Instruct (æ—¥æœ¬èªå¯¾å¿œ)",
            "elyza/ELYZA-japanese-Llama-2-7b-instruct": "ELYZA Japanese Llama 2 7B",
            
            # å¤šè¨€èªå¯¾å¿œãƒ»è‹±èªãƒ¢ãƒ‡ãƒ«ï¼ˆInference APIå¯¾å¿œï¼‰
            "microsoft/DialoGPT-large": "DialoGPT Large (å¯¾è©±ç‰¹åŒ–)",
            "bigscience/bloom-7b1": "BLOOM 7B (å¤šè¨€èª)",
            "mistralai/Mistral-7B-Instruct-v0.2": "Mistral 7B Instruct v0.2",
            "microsoft/DialoGPT-medium": "DialoGPT Medium (å¯¾è©±ç‰¹åŒ–)",
            "HuggingFaceH4/zephyr-7b-beta": "Zephyr 7B Beta (å¯¾è©±ç‰¹åŒ–)",
            "NousResearch/Nous-Hermes-2-Yi-34B": "Nous Hermes 2 Yi 34B",
            "upstage/SOLAR-10.7B-Instruct-v1.0": "SOLAR 10.7B Instruct",
            
            # 70Bã‚¯ãƒ©ã‚¹ï¼ˆPRO/Enterpriseå‘ã‘ï¼‰
            "meta-llama/Meta-Llama-3.1-70B-Instruct": "Llama 3.1 70B Instruct (PRO)",
            "meta-llama/Llama-2-70b-chat-hf": "Llama 2 70B Chat (PRO)",
            "meta-llama/Meta-Llama-3-70B-Instruct": "Llama 3 70B Instruct (PRO)"
        }
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
        self.current_model = "cyberagent/open-calm-7b"
        
        # HuggingFace APIè¨­å®š
        self.api_url = "https://api-inference.huggingface.co/models/"
        self.headers = {}
        
    def set_api_key(self, api_key: str):
        """APIã‚­ãƒ¼ã‚’è¨­å®š"""
        if api_key.strip():
            self.headers = {"Authorization": f"Bearer {api_key}"}
            return "âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ"
        else:
            return "âŒ æœ‰åŠ¹ãªAPIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    
    def set_model(self, model_name: str):
        """ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´"""
        self.current_model = model_name
        return f"ãƒ¢ãƒ‡ãƒ«ã‚’ {self.models[model_name]} ã«å¤‰æ›´ã—ã¾ã—ãŸ"
    
    def query_model(self, prompt: str, max_length: int = 200, temperature: float = 0.7) -> str:
        """HuggingFace Inference APIã«ã‚¯ã‚¨ãƒªã‚’é€ä¿¡"""
        if not self.headers:
            return "âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        url = self.api_url + self.current_model
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": max_length,
                "temperature": temperature,
                "do_sample": True,
                "top_p": 0.95,
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get("generated_text", "")
                    return generated_text.strip()
                else:
                    return "âŒ äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§ã™"
            elif response.status_code == 503:
                return "â³ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¿ä¸­ã§ã™ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
            elif response.status_code == 401:
                return "âŒ APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™"
            else:
                return f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code})"
                
        except requests.exceptions.Timeout:
            return "â³ ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        except requests.exceptions.RequestException as e:
            return f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def chat_response(self, message: str, history: List[Tuple[str, str]], 
                     max_length: int, temperature: float) -> Tuple[str, List[Tuple[str, str]]]:
        """ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’ç”Ÿæˆ"""
        if not message.strip():
            return "", history
        
        # å¯¾è©±å±¥æ­´ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        conversation_context = ""
        for user_msg, bot_msg in history[-3:]:  # ç›´è¿‘3å›ã®ä¼šè©±ã‚’å«ã‚ã‚‹
            conversation_context += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_msg}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {bot_msg}\n"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰ï¼ˆãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰
        if "weblab" in self.current_model.lower() or "matsuo-lab" in self.current_model.lower():
            # WebLabç”¨ã®æŒ‡ç¤ºå½¢å¼
            prompt = f"ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã¨ã€æ–‡è„ˆã®ã‚ã‚‹å…¥åŠ›ã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n### æŒ‡ç¤º:\næ—¥æœ¬èªã§è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n### å…¥åŠ›:\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\n\n### å¿œç­”:\n"
        elif "rinna" in self.current_model.lower() and "instruction" in self.current_model.lower():
            # RinnaæŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ç”¨
            prompt = f"{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
        elif "elyza" in self.current_model.lower() or "swallow" in self.current_model.lower():
            # ELYZA/Swallowç”¨ã®æŒ‡ç¤ºå½¢å¼
            prompt = f"ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚\n\n### æŒ‡ç¤º:\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\n\n### å¿œç­”:"
        elif "llama" in self.current_model.lower() and ("chat" in self.current_model.lower() or "instruct" in self.current_model.lower()):
            # Llama Chat/Instructç”¨ã®æŒ‡ç¤ºå½¢å¼
            if "llama-3" in self.current_model.lower():
                # Llama 3ã‚·ãƒªãƒ¼ã‚ºç”¨
                prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            else:
                # Llama 2ã‚·ãƒªãƒ¼ã‚ºç”¨
                prompt = f"<s>[INST] {conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message} [/INST]"
        elif "mistral" in self.current_model.lower() or "zephyr" in self.current_model.lower():
            # Mistral/Zephyrç”¨ã®æŒ‡ç¤ºå½¢å¼
            prompt = f"<s>[INST] {conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message} [/INST]"
        elif "nous" in self.current_model.lower():
            # Nous Hermesç”¨ã®æŒ‡ç¤ºå½¢å¼
            prompt = f"### Instruction:\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\n\n### Response:"
        elif "solar" in self.current_model.lower():
            # SOLARç”¨ã®æŒ‡ç¤ºå½¢å¼
            prompt = f"### User:\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\n\n### Assistant:"
        elif "instruct" in self.current_model.lower():
            # ä¸€èˆ¬çš„ãªæŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ç”¨
            prompt = f"ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã¨ã€æ–‡è„ˆã®ã‚ã‚‹å…¥åŠ›ã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n\n### æŒ‡ç¤º:\næ—¥æœ¬èªã§è‡ªç„¶ãªä¼šè©±ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n### å…¥åŠ›:\n{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\n\n### å¿œç­”:\n"
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå½¢å¼
            prompt = f"{conversation_context}ãƒ¦ãƒ¼ã‚¶ãƒ¼: {message}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"
        
        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¿œç­”ã‚’å–å¾—
        response = self.query_model(prompt, max_length, temperature)
        
        # å±¥æ­´ã«è¿½åŠ 
        history.append((message, response))
        
        return "", history

# ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
chat_bot = JapaneseLLMChat()

# Gradio ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ§‹ç¯‰
def create_interface():
    with gr.Blocks(
        title="æ—¥æœ¬èªLLMãƒãƒ£ãƒƒãƒˆ",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1000px !important;
        }
        """
    ) as demo:
        
        gr.Markdown(
            """
            # ğŸ¤– æ—¥æœ¬èªLLMãƒãƒ£ãƒƒãƒˆ
            HuggingFace Inference APIã‚’ä½¿ç”¨ã—ãŸæ—¥æœ¬èªå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ 
            """
        )
        
        with gr.Row():
            with gr.Column(scale=2):
                # APIã‚­ãƒ¼è¨­å®š
                with gr.Group():
                    gr.Markdown("### ğŸ”‘ è¨­å®š")
                    api_key_input = gr.Textbox(
                        label="HuggingFace API Token",
                        placeholder="hf_xxxxxxxxxxxxxxxxx",
                        type="password"
                    )
                    api_key_btn = gr.Button("APIã‚­ãƒ¼ã‚’è¨­å®š", variant="primary")
                    api_key_status = gr.Textbox(label="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", interactive=False)
                
                # ãƒ¢ãƒ‡ãƒ«é¸æŠ
                with gr.Group():
                    gr.Markdown("### ğŸ§  ãƒ¢ãƒ‡ãƒ«é¸æŠ")
                    model_dropdown = gr.Dropdown(
                        choices=[(v, k) for k, v in chat_bot.models.items()],
                        value="cyberagent/open-calm-7b",
                        label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«"
                    )
                    model_status = gr.Textbox(label="ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«", interactive=False, 
                                            value=chat_bot.models[chat_bot.current_model])
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
                    max_length_slider = gr.Slider(
                        minimum=50, maximum=500, value=200,
                        label="æœ€å¤§ç”Ÿæˆé•·"
                    )
                    temperature_slider = gr.Slider(
                        minimum=0.1, maximum=2.0, value=0.7,
                        label="Temperatureï¼ˆå‰µé€ æ€§ï¼‰"
                    )
            
            with gr.Column(scale=3):
                # ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
                chatbot = gr.Chatbot(
                    height=500,
                    label="ä¼šè©±",
                    show_label=True,
                    avatar_images=["ğŸ‘¤", "ğŸ¤–"]
                )
                
                msg = gr.Textbox(
                    label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                    placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                    lines=2
                )
                
                with gr.Row():
                    send_btn = gr.Button("é€ä¿¡", variant="primary")
                    clear_btn = gr.Button("ä¼šè©±ã‚’ã‚¯ãƒªã‚¢", variant="secondary")
        
        # ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨æ–¹æ³•", open=False):
            gr.Markdown(
                """
                1. **APIã‚­ãƒ¼ã®è¨­å®š**: HuggingFaceï¼ˆhttps://huggingface.co/settings/tokensï¼‰ã‹ã‚‰Access Tokenã‚’å–å¾—ã—ã€ä¸Šè¨˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„
                2. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã—ãŸã„æ—¥æœ¬èªLLMã‚’é¸æŠã—ã¦ãã ã•ã„
                3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: å¿…è¦ã«å¿œã˜ã¦ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
                4. **ãƒãƒ£ãƒƒãƒˆé–‹å§‹**: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„
                
                **æ³¨æ„**: 
                - åˆå›ä½¿ç”¨æ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
                - 70Bã‚¯ãƒ©ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆPROè¡¨ç¤ºï¼‰ã¯ HuggingFace PRO ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒå¿…è¦ã§ã™
                - Inference APIéå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ï¼ˆSarashina2-70Bç­‰ï¼‰ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“
                - ãƒªã‚¹ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯Inference APIå¯¾å¿œã‚’ç¢ºèªæ¸ˆã¿ã§ã™
                - 70Bãƒ¢ãƒ‡ãƒ«ã¯é«˜ã„ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™
                """
            )
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
        api_key_btn.click(
            chat_bot.set_api_key,
            inputs=[api_key_input],
            outputs=[api_key_status]
        )
        
        model_dropdown.change(
            chat_bot.set_model,
            inputs=[model_dropdown],
            outputs=[model_status]
        )
        
        send_btn.click(
            chat_bot.chat_response,
            inputs=[msg, chatbot, max_length_slider, temperature_slider],
            outputs=[msg, chatbot]
        )
        
        msg.submit(
            chat_bot.chat_response,
            inputs=[msg, chatbot, max_length_slider, temperature_slider],
            outputs=[msg, chatbot]
        )
        
        clear_btn.click(
            lambda: ([], ""),
            outputs=[chatbot, msg]
        )
    
    return demo

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860
    )